{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/josephepstein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Scraping\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import PyQt5\n",
    "%matplotlib qt\n",
    "\n",
    "# ML Things\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "RE_PUNCT = re.compile('([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\", \"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "\n",
    "    sentence = RE_PUNCT.sub(\" \", sentence).lower().split()\n",
    "    return_sentence = sentence\n",
    "        \n",
    "    for word in sentence:\n",
    "        if word in STOP_WORDS or len(word) < 3:\n",
    "            return_sentence = list(filter((word).__ne__, return_sentence))\n",
    "    return return_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(split_sentences):\n",
    "    word_count_dict = {}\n",
    "    \n",
    "    for sentence in split_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_count_dict:\n",
    "                word_count_dict[word] = 0\n",
    "        \n",
    "            word_count_dict[word] += 1\n",
    "        \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blog_scraper( blog_url, post_link_selector, post_content_selector):\n",
    "    \n",
    "    post_text_array = []\n",
    "    \n",
    "    pages = np.arange(2, 50)\n",
    "\n",
    "    for page in pages:\n",
    "        \n",
    "        blog = requests.get( blog_url + str(page), headers=headers)\n",
    "\n",
    "        soup = BeautifulSoup(blog.text, 'html.parser')\n",
    "\n",
    "        post_links = soup.select( post_link_selector )\n",
    "\n",
    "        for post_link in post_links:\n",
    "                        \n",
    "            if len(post_text_array) >= 100:\n",
    "                \n",
    "                return post_text_array\n",
    "            \n",
    "            blog = requests.get(post_link['href'], headers=headers)\n",
    "            \n",
    "            post_soup = BeautifulSoup(blog.text, 'html.parser')\n",
    "            \n",
    "            post_text = post_soup.select_one( post_content_selector ).text\n",
    "                        \n",
    "            post_text_array.append(post_text)\n",
    "    \n",
    "    return post_text_array\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_plot(model, vocab, top_n=-1, plot=PCA):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    temp_vocab = vocab.copy()\n",
    "    \n",
    "    if top_n != -1:\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for counter in range(0, top_n):\n",
    "            tokens.append(model[max(temp_vocab, key=temp_vocab.get)])\n",
    "            labels.append(max(temp_vocab, key=temp_vocab.get))\n",
    "            temp_vocab.pop(max(temp_vocab, key=temp_vocab.get))\n",
    "    else:\n",
    "        for word in model.wv.vocab:\n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "        \n",
    "    if plot == PCA:\n",
    "        new_values = PCA(random_state=23).fit_transform(tokens)[:,:2]\n",
    "    else:\n",
    "        tsne_model = TSNE(perplexity=100, n_components=2, init='pca', n_iter=1000, random_state=23)\n",
    "        new_values = tsne_model.fit_transform(tokens)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasciencecentral_url = 'https://www.datasciencecentral.com/profiles/blog/list?promoted=1&page='\n",
    "datasciencecentral_link_selector = '#xg .xg_module_body .title a:nth-child(2)'\n",
    "datasciencecentral_content_selector = '#xg .postbody'\n",
    "\n",
    "smartdatacollective_url = 'https://www.smartdatacollective.com/page/'\n",
    "smartdatacollective_link_selector = '.content-inner .p-outer .p-footer a.btn'\n",
    "smartdatacollective_content_selector = '.single-content .entry-content'\n",
    "\n",
    "starbridgepartners_url = 'https://starbridgepartners.com/data-science-report/page/'\n",
    "starbridgepartners_link_selector = 'article.post .entry-title-link'\n",
    "starbridgepartners_content_selector = 'article'\n",
    "\n",
    "data_science_blogs = blog_scraper( datasciencecentral_url, datasciencecentral_link_selector, datasciencecentral_content_selector )\n",
    "data_science_blogs.extend(blog_scraper( smartdatacollective_url, smartdatacollective_link_selector, smartdatacollective_content_selector ))\n",
    "data_science_blogs.extend(blog_scraper( starbridgepartners_url, starbridgepartners_link_selector, starbridgepartners_content_selector ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_blogs_split = [clean_sentence(row) for row in data_science_blogs]\n",
    "\n",
    "word_count_dict = count_words(data_science_blogs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(data_science_blogs_split, min_count=1,size= 50,workers=3, window =3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "word_vector_plot(model, word_count_dict, 10, plot=TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
